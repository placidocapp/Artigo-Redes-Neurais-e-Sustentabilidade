{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybtex.database.input import bibtex\n",
    "import csv\n",
    "import unidecode\n",
    "\n",
    "# Abre o arquivo bib\n",
    "parser = bibtex.Parser()\n",
    "bibdata = parser.parse_file(\"scopus.bib\")\n",
    "\n",
    "# Itera entre as referências\n",
    "dados = []\n",
    "\n",
    "for bib_id in bibdata.entries:\n",
    "    b = bibdata.entries[bib_id].fields\n",
    "    dados_dict = {}\n",
    "    \n",
    "    try:\n",
    "        # change these lines to create a SQL insert\n",
    "        dados_dict['Titulo'] = b[\"title\"]\n",
    "        #print(b[\"title\"])\n",
    "    except:\n",
    "        print(\"Não conseguiu ler o Título no artigo \" + str(b) )\n",
    "        \n",
    "    try:\n",
    "        # change these lines to create a SQL insert\n",
    "        dados_dict['Journal'] = b[\"journal\"]\n",
    "        #print(b[\"journal\"])\n",
    "    except:\n",
    "        print(\"Não conseguiu ler o journal no artigo \" + str(b) )\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        # change these lines to create a SQL insert\n",
    "        dados_dict['Ano'] = b[\"year\"]\n",
    "        #print(b[\"year\"])\n",
    "    except:\n",
    "        print(\"Não conseguiu ler o year no artigo \" + str(b) )\n",
    "        \n",
    "    try:\n",
    "        # change these lines to create a SQL insert\n",
    "        dados_dict['Tipo'] = b[\"document_type\"]\n",
    "        #print(b[\"document_type\"])\n",
    "    except:\n",
    "        print(\"Não conseguiu ler o document_type no artigo \" + str(b) )\n",
    "        \n",
    "    try:\n",
    "        # change these lines to create a SQL insert\n",
    "        dados_dict['Fonte'] = b[\"source\"]\n",
    "        #print(b[\"source\"])\n",
    "    except:\n",
    "        print(\"Não conseguiu ler o source no artigo \" + str(b) )\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        #print(b)\n",
    "        dados_dict['Autores'] = []\n",
    "        # Múltiplos autores\n",
    "        for author in bibdata.entries[bib_id].persons[\"author\"]:\n",
    "            dados_dict['Autores'].append(''.join(author.last_names + [' '] + author.first_names))\n",
    "        #    print(author.first_names, author.last_names)\n",
    "    except:\n",
    "        print(\"Não conseguiu ler os autores no artigo \" + str(b) )\n",
    "        \n",
    "    # Salva em uma variável\n",
    "    dados.append(dados_dict)\n",
    "    \n",
    "#Coloca em um xls ----------------------------------------------------------------------------------\n",
    "\n",
    "#Escreve o header\n",
    "csv = ''\n",
    "for chave in dados[0]:\n",
    "    csv += chave + \";\"\n",
    "csv += '\\n'\n",
    "\n",
    "for artigo in dados:\n",
    "    for chave in dados[0]:\n",
    "        try:\n",
    "            csv += artigo[chave] + ';'\n",
    "        except:\n",
    "            # Deve ser uma lista de autores\n",
    "            for nome in artigo[chave]:\n",
    "                csv += nome + ' | '\n",
    "            csv += ';'\n",
    "    csv += '\\n'\n",
    "    \n",
    "with open('test.csv', 'w') as f:\n",
    "    f.write(unidecode.unidecode(csv))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extração de bib's na pasta: C:\\Users\\Plácido\\Downloads\\\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Downloads\\10.1007_s10444-018-09658-6.bib\n",
      "Não conseguiu ler o document_type no artigo Regularization theory in the study of generalization ability of a biological neural network model\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Downloads\\10.1007_s11063-018-9883-8.bib\n",
      "Não conseguiu ler o document_type no artigo Enhance the Performance of Deep Neural Networks via L2 Regularization on the Input of Activations\n",
      "Salvou corretamente arquivo: C:\\Users\\Plácido\\Downloads\\Extracao_Downloads.csv\n"
     ]
    }
   ],
   "source": [
    "from pybtex.database.input import bibtex\n",
    "import unidecode\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def letras_espacos(texto):\n",
    "    return ''.join([c for c in texto if c.isalpha() or c.isspace()])\n",
    "\n",
    "# Recebe uma pasta e dentro dela acumula todos os dados de arquivos bib que encontrar\n",
    "def extrai_bib(pasta, source=''):\n",
    "    print(\"Iniciando extração de bib's na pasta: \" + pasta)\n",
    "    \n",
    "    # Flag para indicar se devemos colocar ou não o header\n",
    "    sem_header = True\n",
    "    csv = ''\n",
    "    \n",
    "    # Percorre todos os arquivos da pasta e entra em todos com .bib\n",
    "    for file in os.listdir(pasta):\n",
    "        # Extrai extensão do arquivo\n",
    "        filename, file_ext = os.path.splitext(file)\n",
    "        \n",
    "        # Se for arquivo bib então extrai os dados dele\n",
    "        if file_ext == '.bib':\n",
    "            print(\"Extraindo dados do arquivo \" + pasta + file)\n",
    "            \n",
    "            # Abre o arquivo bib\n",
    "            parser = bibtex.Parser()\n",
    "            bibdata = parser.parse_file(pasta + file)\n",
    "\n",
    "            # Itera entre as referências\n",
    "            dados = []\n",
    "\n",
    "            for bib_id in bibdata.entries:\n",
    "                b = bibdata.entries[bib_id].fields\n",
    "                dados_dict = {}\n",
    "\n",
    "                try:\n",
    "                    # change these lines to create a SQL insert\n",
    "                    dados_dict['Titulo'] = b[\"title\"]\n",
    "                    #print(b[\"title\"])\n",
    "                except:\n",
    "                    dados_dict['Titulo'] = ''\n",
    "                    print(\"Não conseguiu ler o Título no artigo \" + str(b[\"title\"]) )\n",
    "\n",
    "                try:\n",
    "                    # change these lines to create a SQL insert\n",
    "                    dados_dict['Journal'] = b[\"journal\"]\n",
    "                    #print(b[\"journal\"])\n",
    "                except:\n",
    "                    dados_dict['Journal'] = ''\n",
    "                    print(\"Não conseguiu ler o journal no artigo \" + str(b[\"title\"]) )\n",
    "\n",
    "\n",
    "                try:\n",
    "                    # change these lines to create a SQL insert\n",
    "                    dados_dict['Ano'] = b[\"year\"]\n",
    "                    #print(b[\"year\"])\n",
    "                except:\n",
    "                    dados_dict['Ano'] = ''\n",
    "                    print(\"Não conseguiu ler o year no artigo \" + str(b[\"title\"]) )\n",
    "\n",
    "                try:\n",
    "                    # change these lines to create a SQL insert\n",
    "                    dados_dict['Tipo'] = b[\"document_type\"]\n",
    "                    #print(b[\"document_type\"])\n",
    "                except:\n",
    "                    dados_dict['Tipo'] = ''\n",
    "                    print(\"Não conseguiu ler o document_type no artigo \" + str(b[\"title\"]) )\n",
    "\n",
    "                try:\n",
    "                    # change these lines to create a SQL insert\n",
    "                    if source != '':\n",
    "                        dados_dict['Fonte'] = source\n",
    "                    else:\n",
    "                        dados_dict['Fonte'] = b[\"source\"]\n",
    "                    #print(b[\"source\"])\n",
    "                except:\n",
    "                    dados_dict['Fonte'] = ''\n",
    "                    print(\"Não conseguiu ler o source no artigo \" + str(b[\"title\"]) )\n",
    "\n",
    "\n",
    "                try:\n",
    "                    #print(b)\n",
    "                    dados_dict['Autores'] = []\n",
    "                    # Múltiplos autores\n",
    "                    for author in bibdata.entries[bib_id].persons[\"author\"]:\n",
    "                        dados_dict['Autores'].append(letras_espacos(''.join(author.last_names + [' '] + author.first_names)))\n",
    "                    #    print(author.first_names, author.last_names)\n",
    "                except:\n",
    "                    dados_dict['Autores'] = []\n",
    "                    print(\"Não conseguiu ler os autores no artigo \" + str(b[\"title\"]) )\n",
    "\n",
    "                # Salva em uma variável\n",
    "                dados.append(dados_dict)\n",
    "\n",
    "            #Coloca em um xls ----------------------------------------------------------------------------------\n",
    "\n",
    "            #Escreve o header\n",
    "            if sem_header == True:\n",
    "                sem_header = False\n",
    "                csv = ''\n",
    "                for chave in dados[0]:\n",
    "                    csv += chave + \";\"\n",
    "                csv += '\\n'\n",
    "\n",
    "            for artigo in dados:\n",
    "                for chave in dados[0]:\n",
    "                    try:\n",
    "                        csv += artigo[chave] + ';'\n",
    "                    except:\n",
    "                        # Deve ser uma lista de autores\n",
    "                        for nome in artigo[chave]:\n",
    "                            csv += nome + ' | '\n",
    "                        csv += ';'\n",
    "                csv += '\\n'\n",
    "\n",
    "    with open(pasta + 'Extracao_' + pasta.split('\\\\')[-2] + '.csv', 'w') as f:\n",
    "        f.write(unidecode.unidecode(csv))\n",
    "        \n",
    "    print(\"Salvou corretamente arquivo: \" + pasta + 'Extracao_' + pasta.split('\\\\')[-2] + '.csv')\n",
    "         \n",
    "pasta = 'C:\\\\Users\\\\Plácido\\\\Downloads\\\\'\n",
    "extrai_bib(pasta, 'Elsevier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extração de bib's na pasta: C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_3-540-57369-0_31.bib\n",
      "Não conseguiu ler o journal no artigo Regularization learning of neural networks for generalization\n",
      "Não conseguiu ler o document_type no artigo Regularization learning of neural networks for generalization\n",
      "Não conseguiu ler o source no artigo Regularization learning of neural networks for generalization\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_3-540-61510-5_95.bib\n",
      "Não conseguiu ler o journal no artigo Bayesian regularization in constructive neural networks\n",
      "Não conseguiu ler o document_type no artigo Bayesian regularization in constructive neural networks\n",
      "Não conseguiu ler o source no artigo Bayesian regularization in constructive neural networks\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_978-3-319-11758-4_31.bib\n",
      "Não conseguiu ler o journal no artigo DropAll: Generalization of Two Convolutional Neural Network Regularization Methods\n",
      "Não conseguiu ler o document_type no artigo DropAll: Generalization of Two Convolutional Neural Network Regularization Methods\n",
      "Não conseguiu ler o source no artigo DropAll: Generalization of Two Convolutional Neural Network Regularization Methods\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_978-3-319-44781-0_10.bib\n",
      "Não conseguiu ler o journal no artigo The Effects of Regularization on Learning Facial Expressions with Convolutional Neural Networks\n",
      "Não conseguiu ler o document_type no artigo The Effects of Regularization on Learning Facial Expressions with Convolutional Neural Networks\n",
      "Não conseguiu ler o source no artigo The Effects of Regularization on Learning Facial Expressions with Convolutional Neural Networks\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_978-3-319-71078-5_11.bib\n",
      "Não conseguiu ler o journal no artigo Quantization Error-Based Regularization in Neural Networks\n",
      "Não conseguiu ler o document_type no artigo Quantization Error-Based Regularization in Neural Networks\n",
      "Não conseguiu ler o source no artigo Quantization Error-Based Regularization in Neural Networks\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_978-981-10-8527-7_30.bib\n",
      "Não conseguiu ler o journal no artigo A Comparative Analysis of Various Regularization Techniques to Solve Overfitting Problem in Artificial Neural Network\n",
      "Não conseguiu ler o document_type no artigo A Comparative Analysis of Various Regularization Techniques to Solve Overfitting Problem in Artificial Neural Network\n",
      "Não conseguiu ler o source no artigo A Comparative Analysis of Various Regularization Techniques to Solve Overfitting Problem in Artificial Neural Network\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_s00521-014-1730-x.bib\n",
      "Não conseguiu ler o document_type no artigo Batch gradient training method with smoothing {\\$}{\\$}{\\backslash}boldsymbol{\\{}{\\backslash}ell{\\}}{\\_}{\\{}{\\backslash}bf 0{\\}}{\\$}{\\$}ℓ0regularization for feedforward neural networks\n",
      "Não conseguiu ler o source no artigo Batch gradient training method with smoothing {\\$}{\\$}{\\backslash}boldsymbol{\\{}{\\backslash}ell{\\}}{\\_}{\\{}{\\backslash}bf 0{\\}}{\\$}{\\$}ℓ0regularization for feedforward neural networks\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_s10444-018-09658-6.bib\n",
      "Não conseguiu ler o document_type no artigo Regularization theory in the study of generalization ability of a biological neural network model\n",
      "Não conseguiu ler o source no artigo Regularization theory in the study of generalization ability of a biological neural network model\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1007_s11063-018-9883-8.bib\n",
      "Não conseguiu ler o document_type no artigo Enhance the Performance of Deep Neural Networks via L2 Regularization on the Input of Activations\n",
      "Não conseguiu ler o source no artigo Enhance the Performance of Deep Neural Networks via L2 Regularization on the Input of Activations\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\10.1023_A_1015292818897.bib\n",
      "Não conseguiu ler o document_type no artigo The Equivalence of Support Vector Machine and Regularization Neural Networks\n",
      "Não conseguiu ler o source no artigo The Equivalence of Support Vector Machine and Regularization Neural Networks\n",
      "Extraindo dados do arquivo C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\ScienceDirect_citations_1561302348388.bib\n",
      "Não conseguiu ler o document_type no artigo DropWeak: A novel regularization method of neural networks\n",
      "Não conseguiu ler o source no artigo DropWeak: A novel regularization method of neural networks\n",
      "Não conseguiu ler o document_type no artigo A novel one-layer recurrent neural network for the l1-regularized least square problem\n",
      "Não conseguiu ler o source no artigo A novel one-layer recurrent neural network for the l1-regularized least square problem\n",
      "Não conseguiu ler o document_type no artigo Neural network for constrained nonsmooth optimization using Tikhonov regularization\n",
      "Não conseguiu ler o source no artigo Neural network for constrained nonsmooth optimization using Tikhonov regularization\n",
      "Não conseguiu ler o document_type no artigo L1/2 regularization learning for smoothing interval neural networks: Algorithms and convergence analysis\n",
      "Não conseguiu ler o source no artigo L1/2 regularization learning for smoothing interval neural networks: Algorithms and convergence analysis\n",
      "Não conseguiu ler o document_type no artigo Smooth group L1/2 regularization for input layer of feedforward neural networks\n",
      "Não conseguiu ler o source no artigo Smooth group L1/2 regularization for input layer of feedforward neural networks\n",
      "Não conseguiu ler o document_type no artigo Efficient construction of sparse radial basis function neural networks using L1-regularization\n",
      "Não conseguiu ler o source no artigo Efficient construction of sparse radial basis function neural networks using L1-regularization\n",
      "Não conseguiu ler o document_type no artigo SRNN: Self-regularized neural network\n",
      "Não conseguiu ler o source no artigo SRNN: Self-regularized neural network\n",
      "Não conseguiu ler o document_type no artigo A novel companion objective function for regularization of deep convolutional neural networks\n",
      "Não conseguiu ler o source no artigo A novel companion objective function for regularization of deep convolutional neural networks\n",
      "Não conseguiu ler o document_type no artigo DropELM: Fast neural network regularization with Dropout and DropConnect\n",
      "Não conseguiu ler o source no artigo DropELM: Fast neural network regularization with Dropout and DropConnect\n",
      "Não conseguiu ler o document_type no artigo Deep neural networks regularization for structured output prediction\n",
      "Não conseguiu ler o source no artigo Deep neural networks regularization for structured output prediction\n",
      "Não conseguiu ler o document_type no artigo Regularization of deep neural networks with spectral dropout\n",
      "Não conseguiu ler o source no artigo Regularization of deep neural networks with spectral dropout\n",
      "Não conseguiu ler o document_type no artigo Group sparse regularization for deep neural networks\n",
      "Não conseguiu ler o source no artigo Group sparse regularization for deep neural networks\n",
      "Não conseguiu ler o document_type no artigo Eigenvalue decay: A new method for neural network regularization\n",
      "Não conseguiu ler o source no artigo Eigenvalue decay: A new method for neural network regularization\n",
      "Salvou corretamente arquivo: C:\\Users\\Plácido\\Documents\\Faculdade\\Andre\\TG\\Artigos\\Artigos para analise\\Extracao_Artigos para analise.csv\n"
     ]
    }
   ],
   "source": [
    "extrai_bib('C:\\\\Users\\\\Plácido\\\\Documents\\\\Faculdade\\\\Andre\\\\TG\\\\Artigos\\\\Artigos para analise\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
